% !TeX spellcheck = en_GB

\newenvironment{scenario}[2]{%
	\newcommand{\case}[2]{%
			\paragraph{Beaviour in case ##1} \hfill \\
			##2
	}%
	\subsection{#2}\label{scenario:#1}
}{
}

\section{Scenarios}
\label{scenarios}
This document describes various scenarios of system usage and possible failures.

\begin{scenario}{create-backup}{Create backup}
    The client wants to create a backup.
    
    \begin{enumerate}
    	\item Client asks the management for a list of \emph{node}s. The \emph{management} returns a sorted list of all nodes (message: \emph{client fetch target nodes}) (sorting might be based on specific configuration).
    		\begin{itemize}
    			\item If the management is down and this is a first time backup, client records message and aborts.
   				\item If the management is down, client tries the same host configuration as last time.
   			\end{itemize}
   		\item Client tries to contact nodes in the presorted order.
   			\begin{enumerate}
   				\item The client sends a \emph{client backup request}. %TODO: Define backup request with estimate, identification, how long the data should be stored
   				\item The node acknowledges or denies the backup request.
   				\item If the node acknowledges, it is declared as \emph{designated node}
   				\item If the node denies the backup request, try next node.
   				\item If no node answers the request, client records an error message and aborts.
   			\end{enumerate}
   		\item If the contact was established, the client starts creating a backup.
   			\begin{enumerate}
   				\item Split all files that changed since the last backup into \emph{chunk}s\footnote{using a rolling hash, see \url{https://borgbackup.readthedocs.io/en/stable/internals/data-structures.html\#chunks}} and calculate a corresponding hash, the \emph{chunk identifier} and add it to the local \emph{chunk index}.
   				\item Files that have not changed since the last backup, must already be present in the chunk index.
   				\item Send all chunk identifiers present in the chunk index combined with an \emph{expiration date} to the designated node. (message: \emph{client sends chunk list})
   				\item The designated node checks, if all chunks received from the client chunk list are present on the node.
   					\begin{itemize}
   						\item If a chunk is already present, update its expiration date if it is further in the future.
   						\item If a chunk is not present, request it from the client (see message response \emph{client sends chunk list} for further details)
   					\end{itemize}
   				\item The client sends the requested chunks to the designated node with a \emph{client sends chunks} message. %TODO: Abschluss client transaktion (root handle)
   					\begin{enumerate}
   						\item The designated node verifies and persists the chunk into its \emph{storage}. Afterwards, it acknowledges receipt to the client (see \emph{client sends chunks} response message).
   						   			%TODO: Client must remember, which chunks have been acknowledged. This could be done with a sliding window for QoS/memory usage?
   						\item The designated node replicates chunks in a continuous replication process. (See scenario \ref{scenario:data-replication})
   					\end{enumerate}
   				\item The client creates a \emph{serialised metadata chunks}. %TODO: Serialised Metadata is the chunk index plus backup metadata as linked list
   				\item The client sends the additional chunks (as \emph{client sends chunk list} messages) to the designated node, in which the \emph{root handle} is highlighted. %TODO: The root handle holds a reference to the first list chunk as well as owner etc. information.
   			\end{enumerate}
    \end{enumerate}
    
    \subsubsection{Special cases}
 	\begin{itemize}
	 	\item If a client is suspended while running, it continuous with the backup process on resume. %TODO: If the expiration date is in the past, the node should refuse the chunk.
	 	\item If a file is changed while the backup is running, and the chunk cannot be created, the backup must be restarted. %TODO: This may lead to starvation.
		\item If client crashes, the backup is aborted and won't be continued if the client is restarted.
		\item If the node goes away (disconnects/crashes/shuts down) during the backup, the client tries to resume the process. After a certain time (e.g. 15m), the client gives up and restarts the backup process from the beginning.
		\item The node must reject (i.e. not acknowledge) a chunk, if the creation date is more than e.g. an hour in the future or past to prevent data loss on bad synchronized clocks.
		\item If the node runs out of storage capacity, it does reject further data chunks. After a timeout, the client restarts the backup process (with another designated node).
	\end{itemize}
	
	\subsubsection{Possible simplifications in this study project}
	\begin{description}
		\item[3a)] Don't split files into chunks, send them as is.
	\end{description}
    
\end{scenario}


\begin{scenario}{backup-restore}{Backup Restore}
	The client wants to restore specific data.
	
	\begin{enumerate}
		\item See \ref{scenario:create-backup} step 1.
		\item The client contacts nodes in the presorted order
			\begin{enumerate}
				\item The client sends a \emph{client list root handles} requests to a node.
				\item The node returns all root handles present in the system (see \emph{client list root handles} response message)
				\item If no node answers the request, client records an error message and aborts.
			\end{enumerate}
		\item The client fetches all \emph{serialised metadata chunks} from the chosen node (message: \emph{client fetch chunks}) and reassembles \emph{chunk index} and backup metadata.
		\item The user specifies which files in which backup version shall be restored. %TODO: Use snapshot as single backup name?
		\item Based on the specified files, the client looks up the \emph{chunk identifiers} in the corresponding \emph{chunk index}.
		\item The client requests the \emph{chunks} from the selected node. (message: \emph{client fetch chunks})
		\item The client reassembles the chunks into files.
	\end{enumerate}
	
	\subsubsection{Special cases}
	\begin{itemize}
		\item If the selected node does not hold the requested chunk, it requests it recursively.
		\item If the client crashes, the whole restore process must be repeated
		\item If the selected node is unavailable, the client selects a new node after a certain timeout (e.g. 5m)
	\end{itemize}
    
	\subsubsection{Possible simplifications in this study project}
	\begin{description}
		\item[-] The client must request a node that has the data already available (possibly the node where the backup was created).
	\end{description}
\end{scenario}


\begin{scenario}{node-join}{Node joining}
    A new node joins the system
    
    \begin{enumerate}
    	\item The node is registered by an administrator in the \emph{managment} with a \emph{location}.
    	\item The new node queries information on himself and all other nodes from the management on startup.
    		\begin{itemize}
    			\item If the management has no information available (yet) or is unavailable, the node retries after a certain timeout (e.g. 5min).
    		\end{itemize}
    	\item Other nodes learn about the new node, as soon as they update their management information.
    		\begin{enumerate}
    			\item If the new host contacts an existing node, before the existing node updates its configuration, the existing node should query the management.
    		\end{enumerate}
    	\item The new node starts to send rumour/information on his state to the existing nodes.
    \end{enumerate}
    
	\subsubsection{Possible simplifications in this study project}
	\begin{description}
		 \item[4)] The rumour data is centralised on the management.
	\end{description}
\end{scenario}

\begin{scenario}{node-leave-planned}{Node leaving planned}
    A node leaves the system planned
    \begin{enumerate}
    	\item The node is removed from the management and disconnected from the network.
    	\item Nodes must start the redundancy recovery process without this node.
    		Also see \ref{scenario:data-replication}.4
    	\item Trough rumour or management updates, all hosts will be informed that this host is currently unavailable.
    \end{enumerate}
    
	\subsubsection{Possible simplifications in this study project}
		See simplifications in \ref{scenario:data-replication}
\end{scenario}

\begin{scenario}{node-leave-unplanned}{Node leaving unplanned}
    A node leaves the system unexpectedly.

	This is basically the same process as in \ref{scenario:node-leave-planned}, but additionally:
	\begin{enumerate}
		\item The management sends a notification to the administrator, after the node has not sent monitoring data after the specified time.
		\item If the node returns, it should first update its management information and thereafter continue with it's replication process.
	\end{enumerate}
\end{scenario}

\begin{scenario}{data-replication}{Data is replicated}
	The network distributes backup data.
   	\begin{enumerate}
   		\item If the node has no recent (<1h) network information, it tries to update it from the management. If that fails, continue with already known nodes and information from rumour.
   		\item Choose best suited node(s) for replication (not same zone, weighting factors as capacity and zone. This data is exchanged in a rumour like manner.)
   		\item Ask peer node, if he already has data chunks (by hash) and keep date.
   			\begin{itemize} %TODO: This could be done more space efficiently.
   				\item If the peer node has the chunk, it must update its keep date and acknowledge.
   				\item If the peer node doesn't have the chunk, it must notify the requesting node, which sends the chunk and keep date.
   				\item If the peer node receives a incomplete or currupted chunk, it must not acknowledge the chunk and notify the sending node.
   				\item If the peer node runs out of capacity while receiving a chunk, it must not acknowledge the chunk and notify the sending node.
   			\end{itemize}
   		\item Every node that has a chunk, must periodically assure that all hosts have it (that should).
   			\begin{itemize}
	   			\item If this is not the case, the noticing node selects additional node(s) that should hold the data, informs all other responsible nodes for this chunks, and starts replicating data to this new node. (Note that this may happen in parallel to other nodes that hold the same data).
	   		\end{itemize}
	   	%TODO: Delete data that was replicated too much?
   	\end{enumerate}
        
     \subsubsection{Possible simplifications in this study project}
     
     \begin{description}
	     \item[2)] The hosts do not exchange information over their state nor do availability checks. This data is gathered and shared centralised via the management
	     \item[-] The data is always distributed on all hosts, not selectively.
	     \item[-] The replication does not take place in an equal manner; the host, which receives the backup, is the only one that does replication and redundancy checks.
     \end{description}
\end{scenario}

\begin{scenario}{data-expiration}{Data has expired lifetime}
    A node wants to delete data associated with an expired backup/snapshot.
    
    \begin{enumerate}
	    \item If a node wants to delete data chunks, it must notify and receive an acknowledge from all other nodes, that hold a data chunk.
	    	\begin{itemize}
	    		\item If the time of a host is off, he is hindered by or hindering the deletion of chunks. Thus, even if a host has a time offset, data is never deleted too early.
	    		\item If a node that replicates a specific data set is not available, the data may not be deleted.
	    	\end{itemize}
	    \item After deleting a data chunk, the node must inform all other nodes that hold a chunk, that this data was deleted.
    \end{enumerate}
    
    
    \subsubsection{Possible simplifications in this study project}
   	In a first prototype, the system may just delete chunks after the specified keep date without further checks.
\end{scenario}

\begin{scenario}{storage-errors}{Data Storage has errors}
    The storage layer notifies the node that some of the data stored on it is corrupt.
    
    \begin{enumerate}
    	\item The storage informs the node of a corrupt data set
    	\item The node requests the chunk from a peer node that has the chunk, and overwrites it locally.
    	\item The node informs the management of a possible storage failure.
    \end{enumerate}
    
    \subsubsection{Possible simplifications in this study project}
    In a first prototype, this feature might not be implemented.
\end{scenario}

\begin{scenario}{network-errors}{Network availability problems}
   	\case{a node can reach only some nodes directly}{
   		The still available nodes should try to satisfy redundancy needs as good as possible. Unavailable nodes should still be listed as data-holders. This may lead to over-replicated data.
   	}
    \case{the network gets partitioned}{
    	(, and the nodes in each partition can only reach each other and not the other ones.)
    	
    	See first case in \ref{scenario:network-errors}
   	}
   	\case{nodes have different states of management information}{
   		The replication should still take place, but might lead to duplicated data. In case a node gets a pointer to this new node anywhere, it should update its management information as soon as possible.
   	}
%    \case{the management wants to update the configuration but can't reach certain nodes?}{
%TODO: In the current draft version, I do not see this case as management information is only pulled ^
%    	The management changes should be queried by the host by a certain time.
%    }
\end{scenario}
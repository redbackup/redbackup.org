% !TeX spellcheck = en_GB
\chapter{Discussion and Conclusion}
\label{sec:discussion-and-conclusion}
This chapter contains our achieved results and lessons learned, discusses future work and closes with a conclusion.

\section{Achieved Result}

\subsection{Prototype}

We implemented the basic concepts described in our architecture in a limited form in our prototype. Rust was undoubtedly an excellent choice as a programming language for the prototype regarding robustness and performance but turned out to have a very steep learning curve. The same applies to the used frameworks tokio and Diesel. For this reason, we had to compromise to demonstrate as much of the architecture as possible without spending too much time on learning Rust.

Nevertheless, our implementation is solid and proves that the proposed architecture is robust and can be pursued further.

\subsection{Prototype Test Results}

\paragraph{Unit Tests}
We used test driven development (TDD) to develop the prototype as much as possible, which turned out to work great in the Rust ecosystem.

Some unit tests written for the prototype are not pure unit tests but minimal integration tests. Because Rust is not a traditional object-oriented language, it is not possible to introduce and use interfaces (Traits) in the same way as we were used to from other languages such as Java or C\#. Due to the steep learning curve Rust has, we were not able to fully utilise the corresponding mechanisms.

\paragraph{Integration Tests}

To write comprehensive black box integration tests, we created a testing library written in Python\footnote{\url{https://www.python.org/}}. In this framework, the internals on how to launch and configure \glspl{client} and \glspl{node} is encapsulated in classes. Using this abstraction, we decided to launch \glspl{client} and \glspl{node} in separate Docker\footnote{\url{https://www.docker.com/}} containers, so that they are as isolated as possible. All containers used in a test case are connected to a dedicated Docker network, which eliminates possible interferences with other network services.

We wrote integration tests that verify that a backup is flawlessly created, restored and replicated onto other \glspl{node}.

\paragraph{Test coverage}

Because Rust is still a young language with a relatively small ecosystem, tools for measuring code quality are still rare and immature. For our unit tests, we used Tarpaulin\footnote{\url{https://github.com/xd009642/tarpaulin}} to generate code coverage. Tarpaulin does not (yet) cover all language features and therefore returns a conservative coverage number. We achieved 53.5\% line coverage. This number would be significantly higher if all executed lines were counted correctly  (e.g. generated code using macros as well as compiler optimisations are not counted).

Code coverage achieved using the integration tests is not yet supported by any tool known to us and therefore undocumented. The integration tests do however cover all positive scenarios that were implemented.

Our integration testing framework allowed us to write such tests in a simple fashion.

\subsection{Architecture}

The architecture we elaborated in Chapter 2 and Appendix \ref{sec:specification} turned out to to be stable. It has paid off that we did not specify too many details at the beginning (for example, the protocol) but focused on the high-level view.

\subsection{Architecture Test Results}

To ensure the validity of the proposed architecture, we manually ran architecture tests. We focused on scalability, data capacity and concurrent backups. We used our integration test framework for these tests as well.

\subsubsection{Overall Performance}\label{sec:overall-performance}

The conducted architecture tests on the prototype have shown a solid overall performance. Unfortunately, we observed significant memory consumption and CPU utilisation during most test scenarios.

\paragraph{CPU Utilisation}
Replication and the creation of a backup require a lot of CPU time on the \gls{node} and \gls{client} components. The \gls{client} components must execute many hash functions during the creation of chunks. A \gls{node} must verify that the provided chunk identifiers can be derived from the sent chunk contents during backup creation and replication.

Intensive CPU utilisation can be problematic, especially on a \gls{node}. This is somewhat an inherit problem of the propsed architecture because these calculation are required to ensure the integrity of the data stored in the system.

To mitigate this issue, a queuing mechanism can be implemented on the \gls{node} component that temporarly accepts chunks without performing integrity checks. These integrity checks can be performed in the near future and the sent chunks can afterwards be added definetly to the chunk table. The backup and replication protocol need to be adapted to signal this temporary queuing on a \gls{node} to \glspl{client} or other \glspl{node}. A new state (e.g. queuing) could be sent instead of an acknowledgement that instructs \glspl{client} and \glspl{node} to ask again for acknowledgement later.

It has to be investigated whether specific CPU acceleration for hash calculation could mitigate this problem as well.

\paragraph{Memory Concumption}
To prevent premature optimisation, which Donald Knuth famously pointed out is the root of all evil \cite{knuth-optimise}, we implemented \glspl{message} without streaming support. We implemented the high-level redbackup protocol as framed Message Pack\footnote{\url{https://msgpack.org/}} encoded \glspl{message} based directly on TCP. Such a Message Pack \gls{message} is (de-)serialised at once. In other words, whenever a \gls{message} is created or received, all its contents including the payload is loaded into memory. This decision, in combination with the design decision to not split up large files into multiple chunks, has led to significant memory concumption.

The protocol details must be clarified in the future to allow \gls{message} streaming. Refactoring the protocol component to use streaming mechanisms is feasible since Tokio provides these mechanisms\cite{tokio-streaming}. 

\subsubsection{Size Scalability Test Results}
As per our requirements in Appendix \ref{requirements}, the architecture should scale up to 100~\glspl{node}.

To test this scenario, we used the same underlying techniques as in our integration tests, but scaled the infrastructure up to 100 \glspl{node}.

Due to the high memory consumption, we were not able to conduct this test with a significant amount of data. A test run during which a 5MB file was replicated to 99 Nodes did not indicate a degraded performance.

\subsubsection{Data Capacity Test Results}
Our requirements (Appendix \ref{requirements}) also state, that a \gls{node} must be able to handle up to e.g. 2TB of data. To test this requirement, we planned to create large amounts of random data that has to be stored. This is a realistic requirement, as e.g. a compressed image, audio and movie collection might reach such sizes in practice.

It was not possible yet to create one single backup of 2TB at once due to the high memory consumption. Performing multiple backups in a row of a smaller dataset (i.e. 5 files with a size of 500MB) has not shown a decrease in performance.

\subsubsection{Concurrent Test Results}

We ran a test in which 5 \glspl{client} back up randomly generated data onto three randomly chosen \glspl{node}. On average, the entire backup process of 1MB data \gls{chunk} took 90-130ms from one docker container into another. These results clearly support our proposed architecture.

In reality, where \glspl{client} and \glspl{node} are on separate physical machines, this time will be significantly higher due to network latency. Also, because the creation of backups require a lot of CPU time, running all \glspl{client} on one machine is somewhat problematic. It is likely, that running all \glspl{client} and \glspl{node} on separate machines would improve the performance slightly.

\section{Lessons Learned}
% Describe what worked well and what went wrong (Lapses)
% What took us time?
% Summarize issues discussed in the retros

In this section, we describe unexpected project events and the lessons we learned from them.

\subsection{Project course}
\subsubsection{Documentation}
While discussing the documentation efforts in mini-retrospective two, we noted that some terms like metadata or chunks were not defined unambiguously and therefore used for different concepts in varying contexts. To standardise these, we decided to introduce a glossary that uniquely and precisely defines each of these terms.

While elaborating the architecture, we started researching advanced data distribution mechanisms and consensus algorithms. We were both very interested in these topics, but after a discussion with Prof.~Mehta and retrospective one, we realised that the time frame of the study project would not suffice to realise such advanced algorithms.

We frequently underestimated the documentation efforts, particularly the time required for reviewing. We responded by estimating more time and increase the risk reserve time for documentation issues. Besides, we also agreed we would stop and reassess earlier on issues that took longer than expected.

\subsubsection{Rust Formatting and Documentation}
During retrospective two, we noted that the source code was not fully formatted according to the Rust Style Guide\footnote{https://github.com/rust-lang-nursery/fmt-rfcs/blob/master/guide/guide.md} and that the source code documentation was not complete. To ensure a consistent code formatting, we added the RustFmt\footnote{https://github.com/rust-lang-nursery/rustfmt\#rustfmt---} tool as acceptance criterion to our Definition of Done \cite{project-plan} and created a task to complete the documentation.

\subsubsection{Project management}
During the first mini-retrospective and first full retrospective, we discussed several small improvements regarding the task management and how, respectively, where we would work together. During the second sprint, we also neglected to plan time for the supervision meetings and infrastructure updates, which we met by creating a checklist for sprint planning.

A month into the project during the second mini-retrospective, we agreed that we should create more issues with shorter running times and make sure that we review issues as soon as possible. Also, the reported working hours were incomplete and only narrowly fulfilled the planned sprint goals. Therefore, we decided to log the working hours more precisely and intensify the work efforts.


\subsection{Decisions}
\subsubsection{Redundancy: \gls{system-m-replication}}
For the prototype, we decided to implement a \gls{system-m-replication} replication. This decision worked out as we expected and allowed us to create a straightforward yet efficient way to replicate \glspl{chunk}.

\subsubsection{Programming Language and Ecosystem}
During the language evaluation, we decided for using Rust to implement the prototype (See \fullref{sec:language-evaluation} for details on this decision).

While we still think, that Rust is the right choice for the implementation of a backup application as presented in this report, we would have been more productive with a language we already had experience in, like Python or Java. For a prototype, these languages would also have sufficed, despite possibly not being as stable and fast as a Rust implementation.

\subsubsection{Frameworks: Tokio and Diesel}
As discussed in Chapter \fullref{sec:our-approach}, we utilised the Tokio and Diesel frameworks. While offering an advanced feature set considered the relative young Rust environment, we found that the documentation for both frameworks was not sufficiently comprehensible.

Also, the Diesel framework offers an insufficient set of type implementations for SQLite and lacks extensibility e.g.~adding support for timezone timestamps.

\subsubsection{Storage: Database with SQLite}
As we started implementing the prototype, using SQLite seemed an obvious choice, as it is both easy to use and lightweight.

This decision turned out to be suboptimal, as SQLite is not very well suited for concurrent write access \cite{sqlite-locking} and offers an insufficient set of data types \cite{sqlite-datatypes}. For example, SQLite only allows signed 32-bit integers to be used as record identifiers, which effectively limits the number of \glspl{file}, folders or \glspl{chunk} to $2^{31}-1$ each in the prototype.

As a result of the combined difficulties with Diesel and SQLite, we spent considerably more time implementing the database access than initially planned.

In hindsight, we should have further evaluated other database systems including an in-memory database for the \gls{client}.

\section{Future work}
% How to procede?

% - Improve the protocol
% - evtl. address the CPU issue
% - implement encryption & Conduct more tests
% - implement joining, leaving & Conduct more tests
% Finish the implementation :D


% pointing out new research lines

\section{Conclusion}
% compare with (papers) problems addressed in the background section
% Final Conclusion - are we happy, did we expect more,...

